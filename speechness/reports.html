<!DOCTYPE html>
	<html lang="en">
	

	<head>
	

	    <meta charset="utf-8">
	    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
	    <meta name="description" content="">
	    <meta name="author" content="">
	

	    <title>Speechness</title>
	

	    <!-- Bootstrap core CSS -->
	    <link href="../vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
	

	    <!-- Custom styles for this template -->
	    <link href="../css/simple-sidebar.css" rel="stylesheet">
	

	</head>
	

	<body>
	

	    <div id="wrapper">
	

	        <!-- Sidebar -->
	        <div id="sidebar-wrapper">
	            <ul class="sidebar-nav">
	                <li>
	                    <a href="#">Home</a>
	                </li>
	                <li>
	                    <a href="https://github.com/AaltoImagingLanguage/speechness">Github</a>
	                </li>
                <li>
                    <a href="reports.html">Supplementary results</a>
                </li>
	                <li>
	                    <a href="contact.html">More information </a>
	                </li>
	                <li>
	                    <a href="../index.html">Back to project menu</a>
	                </li>
	

	            </ul>
	        </div>
	        <!-- /#sidebar-wrapper -->
	        <script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'></script>
	        <!-- Page Content -->
	        <div id="page-content-wrapper">
	            <div class="container-fluid">



<h1>Areas contributing to acoustic decoding of spoken words and environmental sounds show more overlap than areas contributing to semantic decoding of the two classes of sounds</h1>
<p>Successful decoding of the time-evolving spectrogram and phoneme content of spoken words at 100–180 ms lag in the convolution model engaged cortical areas around the bilateral primary auditory cortices, with contributions from superior temporal and inferior frontal areas. Similar areas were involved in decoding the MPS of environmental sounds (at 50–100 ms); 9 out of 20 most predictive areas for acoustic decoding were shared between the two sound groups. 
Semantic decoding of environmental sounds (at 650–700 ms) had strongest contributions from bilateral superior temporal areas, areas surrounding the bilateral central sulci, parietal cortex, as well as right frontal and insular areas. For spoken words, sources that contributed most to the decoding concentrated on areas surrounding occipito-temporal cortex, bilateral frontal cortex, left central sulcus and insular areas. Two out of 20 most predictive areas for semantic decoding were shared between the two sound groups.</p>
<b>Figure: Source_level_decoding_results
</b>
        </div>
<h1>Acoustic similarities of environmental sound sources can enhance their semantic decoding</h1>
<p>Environmental sounds have more acoustic variation than spoken words, and typically more variation across categories than within categories, as sounds with a similar source (e.g. animal vocalizations vs. tool sounds) are more likely to share acoustic features such as frequency content and degree of harmonics or periodicity. This could be reflected in the MEG responses and, in theory, improve the semantic decoding of environmental sounds on category level. To investigate this possibility, we compared the dissimilarity matrices of the semantic features and acoustic features for both stimulus groups with a Mantel test. Some overlap was found in the category structures of acoustic and semantic features for environmental sounds (p = 0.004) but not for spoken words (p = 0.80). This is possibly reflected in the category-level decoding of semantic features that was significantly better than item-level decoding particularly for environmental sounds (Z = 3.4, p = 0.00015). However, the better performance for category-level than item-level decoding was not entirely due to acoustic within-category similarity and between-category differences (in environmental sounds), since this difference was also observed to some extent for spoken words (Z = 2.2, p = 0.025); thus it seems to be partly due to salient cortical representations of the semantic categories.</p>
<b>Figure: Item_vs_category_level_decoding 
</b>
<b>Figure: Feature_similarities 
</b>
        </div>
<h1>Acoustic speaker characteristics influence modulation power spectrum but not spectrogram decoding performance</h1>
<p>We tested whether acoustic characteristics of the 8 different speakers contributed to decoding performance for the spoken words. In within-speaker decoding the two test items are always chosen from the same speaker; in across-speaker-decoding the two test items are always chosen from different speakers. The frequency and modulation content characteristic of each speaker contributed significantly to the MPS-based decoding of spoken words: items spoken by the same speaker could not be distinguished at a very high accuracy (within-speaker decoding of spoken words 58%), whereas the model performed significantly better when only item pairs from different speakers were considered (across-speaker decoding 66%; within-speaker vs. across-speaker decoding: Z = 2.4, p = 0.016). In contrast, the spectrogram convolution model could equally well tell apart items spoken by the same or different speakers (within-speaker decoding 84%, across-speaker decoding 83%; Z = 0.62, p = 0.56). Thus, acoustic features other than the acoustic speaker characteristics play an important role in the decoding of spoken words with the convolution model. </p>
<b>Figure: Within_vs_across_speaker_decoding
</b>
	        </div>
	        <!-- /#page-content-wrapper -->
	

	    </div>
	    <!-- /#wrapper -->
	

	    <!-- Bootstrap core JavaScript -->
	    <script src="../vendor/jquery/jquery.min.js"></script>
	    <script src="../vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
	

	    <!-- Menu Toggle Script -->
	    <script>
	    $("#menu-toggle").click(function(e) {
	        e.preventDefault();
	        $("#wrapper").toggleClass("toggled");
	    }); </script>
	

	</body>
	

	</html>
